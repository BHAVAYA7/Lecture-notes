{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Sean Tulin\n",
    "<br>\n",
    "Date: Feb. 9, 2022\n",
    "<br>\n",
    "PHYS 2030 W22"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=#46769B>Lesson 7: Markov Chain Monte Carlo for continuous distributions</font>\n",
    "\n",
    "## <font color=#46769B>Motivation</font>\n",
    "\n",
    "Let us generalize our discussion from Lesson 6 to the case of probability distribution functions of continuous variables. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=#46769B>Metropolis algorithm for continuous distributions</font>\n",
    "\n",
    "Suppose we have a PDF $P(x)$, where $x$ is a continuous variable that we want to sample. To be concrete, let's consider an exponential function\n",
    "$$P(x) = \\left\\{ \\begin{array}{cc} a e^{-ax} & x > 0 \\\\ 0 & {\\rm otherwise} \\end{array} \\right. \\, .$$ \n",
    "The general procedure for performing an MCMC simulation is generally the same as for a discrete variable. \n",
    "\n",
    "### <font color=#46769B>Choosing your proposal distribution</font>\n",
    "\n",
    "First, we need to decide on a proposal (transition) distribution $Q(x|x^\\prime)$. In the Metropolis algorithm, the proposal distribution is assumed to be symmetric, i.e.,\n",
    "$$Q(x|x^\\prime) = Q(x^\\prime|x) \\, .$$\n",
    "In this course, we will always choose a __normal distribution__ for our proposal distribution: \n",
    "$$Q(x|x^\\prime) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\, e^{-\\frac{(x-x^\\prime)^2}{2 \\sigma^2}} \\, .$$\n",
    "That is, you draw the *new* sample $x$ from a normal distribution centered at the *old* sample $x^\\prime$. \n",
    "This choice for $Q(x|x^\\prime)$ is the most common one for MCMC simulations, in part because it is clearly symmetric (it is invariant under $x \\leftrightarrow x^\\prime$).\n",
    "\n",
    "We still have a choice to make: we are free to choose any value for the width of our proposal distribution, $\\sigma$. There is always a \"sweet spot\" where $\\sigma$ is neither too large nor too small, where your simulation will be optimized.\n",
    "We will gain some experience in how to optimize $\\sigma$ below.\n",
    "\n",
    "The main goal of our MCMC simulation is to obtain a list of $x$ samples\n",
    "$$(x_0, \\, x_1, \\, x_2, \\, ... , \\, x_{N-1})$$\n",
    "that \n",
    "\n",
    "<div>\n",
    "<img src=\"https://github.com/PHYS-2030-Computational-Methods/Lecture-notes/raw/main/figures/MCMC_coin_flip.png\" width=\"600\">\n",
    "</div>\n",
    "\n",
    "- If $\\sigma$ is large, we are more likely to make larger steps in $x$. For example, if $x^\\prime$ was in a region of high probability $P(x^\\prime)$, it is likely that $x$ will be (far) outside that region, with a low probability $P(x)$. Such new points are unlikely to be accepted, and we are likely to reject many points in a row.\n",
    "- If $\\sigma$ is small, we are more likely to make smaller steps in $x$. Since $x$ and $x^\\prime$ are close together, both points have similar probabilities, $P(x) \\approx P(x^\\prime)$, and we are likely to accept the new point $x$. However, \n",
    "\n",
    "\n",
    "### <font color=#46769B>Review: importance sampling</font>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=#46769B>Markov Chain Monte Carlo</font>\n",
    "\n",
    "MCMC is a different algorithm for sampling $x$ from a probability distribution $P(x)$. Before we discuss the algorithm, let's highlight the main differences with importance sampling.\n",
    "- MCMC involves a proposal distribution $Q$ that you are free to choose. Unlike importance sampling, $Q$ *depends* on the previous sample. We write this as: $$Q(x|x^\\prime)$$\n",
    "This the probability to sample $x$ *given* that the previous sample was $x^\\prime$. $Q(x|x^\\prime)$ is known as a __transition probability__.\n",
    "- MCMC generates samples for $x$ that are drawn from the *target* distribution $P(x)$ directly. *No weights are needed.* Importance sampling draws samples for $x$ from the *proposal* distribution and we needed to include weights to calculate quantities with respect to $P(x)$. \n",
    "- In MCMC, after drawing each sample $x_i$, there is an additional step where you must decide whether or not to accept the sample. This is known as the __acceptance/rejection step__.\n",
    "\n",
    "The list of samples for $x$ is called a __chain__. Since each new sample depends on the one before, MCMC is an iterative process where you build your chain one sample at a time until it has the desired length $N$.\n",
    "\n",
    "Now we will go through in detail how (and why!) MCMC works for the coin flip example.\n",
    "\n",
    "### <font color=#46769B>Transition probabilities</font>\n",
    "\n",
    "The first step is to choose your transition probabilities $Q(x|x^\\prime)$. For flipping a coin, the only choice is a Bernoulli distribution, but it will depend on whether the previous sample $x^\\prime$ was heads or tails.\n",
    "\n",
    "Suppose our previous sample is tails ($x^\\prime = 0$). Then we say the probability of getting heads or tails for the next sample $x$ is\n",
    "$$Q(x|0) = \\left\\{ \\begin{array}{cl} q & x=1  \\\\ 1-q & x=0 \\end{array} \\right. \\, .$$\n",
    "Now suppose our previous sample is heads ($x^\\prime = 1$). Then we say the probability of getting heads or tails for the next sample $x$ is\n",
    "$$Q(x|1) = \\left\\{ \\begin{array}{cl} \\bar{q} & x=1  \\\\ 1-\\bar{q} & x=0 \\end{array} \\right. \\, .$$\n",
    "That is, $Q(x|0)$ and $Q(x|1)$ are different distributions. \n",
    "Of course, the total probability for *either* $x=1$ or $x=0$ is always unity, so we must have\n",
    "$$Q(1|0) + Q(0|0) = q + (1-q) = 1 \\, , \\qquad  Q(1|1) + Q(0|1) = \\bar{q} + (1-\\bar{q}) = 1 \\, .$$\n",
    "However, $q$ and $\\bar{q}$ are different in general and we are free to choose them as we like.\n",
    "\n",
    "We can make a map to illustrate all the different possible transitions and their probabilities:\n",
    "<div>\n",
    "<img src=\"https://github.com/PHYS-2030-Computational-Methods/Lecture-notes/raw/main/figures/MCMC_coin_flip.png\" width=\"600\">\n",
    "</div>\n",
    "\n",
    "\n",
    "The most common (and original) type of MCMC is known as the __Metropolis algorithm__. This is the main algorithm we will use in this course. In the Metropolis algorithm, we make an assumption that the transition probability is *symmetric*,\n",
    "$$Q(x|x^\\prime) = Q(x^\\prime|x) \\, ,$$\n",
    "so the probablity of sampling $x^\\prime \\to x$ is the same as sampling $x \\to x^\\prime$.\n",
    "For the coin flip, the Metropolis algorithm assumes $Q(0|1) = Q(1|0)$, i.e., taking a special choice that $\\bar{q} = 1-q$.\n",
    "\n",
    "The more general MCMC setup where $Q(x|x^\\prime) \\ne Q(x^\\prime|x)$ is known as the __Metropolis-Hastings algorithm__.\n",
    "For the coin flip, Metropolis-Hastings simply allows $q$, $\\bar{q}$ to be independent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=#46769B>Acceptance/rejection</font>\n",
    "\n",
    "It is an important step in MCMC algorithms to decide whether to accept or reject a new sample. The process is:\n",
    "- Given a previous sample $x_{i-1}$, generate a new sample $x$ by sampling from $Q(x|x_{i-1})$.\n",
    "- Calculate the acceptance ratio $\\mathcal{A}$, defined below. (Note $\\mathcal{A}$ is always a positive number.)\n",
    "    - If $\\mathcal{A} > 1$, we *always* accept the new point.\n",
    "    - If $\\mathcal{A} < 1$, we *either* accept or reject the new point. We decide to accept or reject randomly: we accept with probability $\\mathcal{A}$ (and therefore we reject with probability $1-\\mathcal{A}$).\n",
    "- The next value in the chain, $x_i$, depends on whether we accept or reject:\n",
    "    - If we accept $x$, we set $x_i = x$, i.e., saving our new sample to the chain.\n",
    "    - If we reject $x$, we set $x_i = x_{i-1}$, i.e., *repeating our previous value in the chain*. \n",
    "    \n",
    "It is easy to forget this last step. Rejection does not mean you try to find a new accepted sample for $x_i$. Rejection means that you *repeat* the previous sample. If you reject ten times in a row, you will be repeating the same values in your chain ten times.\n",
    "\n",
    "For the Metropolis algorithm, the acceptance ratio is\n",
    "$$\\mathcal{A}(x,x^\\prime) = \\frac{P(x)}{P(x^\\prime)}$$\n",
    "where $x$ is the new sample and $x^\\prime$ is the previous sample (i.e., $x_{i-1}$). Thus, if $\\mathcal{A}(x,x^\\prime) > 1$, that means that $x$ is *more probable* than $x^\\prime$. We always keep such points in our sample.\n",
    "\n",
    "On the other hand, we do not *always* reject samples where $\\mathcal{A}(x,x^\\prime) < 1$. \n",
    "For example, let's suppose $\\mathcal{A}(x,x^\\prime) = 0.1$. This means that $x$ is only 10\\% as probable as $x^\\prime$. The procedure described above will accept such samples 10\\% of the time.\n",
    "\n",
    "For the Metropolis-Hastings algorithm, the acceptance ratio is given by a more complicated formula\n",
    "$$\\mathcal{A}(x,x^\\prime) = \\frac{P(x)}{P(x^\\prime)} \\frac{Q(x^\\prime|x)}{Q(x|x^\\prime)} \\, .$$\n",
    "Clearly if $Q(x|x^\\prime) = Q(x^\\prime|x)$, we reduce to the result for the Metropolis algorithm.\n",
    "\n",
    "### <font color=#46769B>Summary</font>\n",
    "\n",
    "Let's summarize how we implement the Metropolis algorithm for flipping a biased coin with $p=0.7$.\n",
    "Before we begin, we need to choose:\n",
    "- An initial value for the chain, $x_0$. Let's choose $x_0 = 0$ (tails).\n",
    "- The transition probability $q$. Let's choose $q=0.5$.\n",
    "- The number of samples $N$ we want in our chain. Let's try $N=10^5$.\n",
    "\n",
    "Next, we do a `for` loop over an index `i` that will generate $N$ entries in our chain. For each iteration in the `for` loop:\n",
    "- Given $x_{i-1}$, generate a new sample $x$ from $Q(x|x_{i-1})$.\n",
    "- Calculate the acceptance ratio $\\mathcal{A} = P(x)/P(x_{i-1})$.\n",
    "- Acceptance/rejection step:\n",
    "    - If accept, set $x_i = x$. \n",
    "    - Else (reject), set $x_i = x_{i-1}$, i.e., repeat the previous value.\n",
    "\n",
    "After $N-1$ iterations, the `for` loop terminates (we started with one sample $x_0$ to begin with). Our chain consists of $N$ samples for $x$:\n",
    "$$x = \\left( x_0, \\, x_1, \\, x_2, \\, ... , \\, x_{N-1}\\right) \\, .$$\n",
    "Now, we can calculate anything we like as if we had sampled from $P(x)$ directly. For example, the mean value of $x$ is\n",
    "$$\\langle x \\rangle = \\frac{1}{N} \\sum_{i=0}^{N-1} x_i \\, .$$\n",
    "The mean value of any function of $x$ is\n",
    "$$\\langle f(x) \\rangle = \\frac{1}{N} \\sum_{i=0}^{N-1} f(x_i) \\, .$$\n",
    "Unlike importance sampling, we do not need to include any weights in our calculations.\n",
    "\n",
    "Here is some code that implements all this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "p = 0.7\n",
    "num = 10**5\n",
    "\n",
    "# Define P(x)\n",
    "# Note: P[0] = 1-p (tails)\n",
    "#       P[1] = p   (heads)\n",
    "\n",
    "P = [1-p, p]\n",
    "\n",
    "# Initialize the first value in the chain [x0]\n",
    "x_samples = [0]\n",
    "\n",
    "for i in range(num-1):\n",
    "    \n",
    "    # Previous value of x\n",
    "    x_old = x_samples[i]\n",
    "    \n",
    "    # Sample new value of x\n",
    "    x_new = np.random.choice([0,1])\n",
    "    \n",
    "    # Acceptance ratio\n",
    "    A = P[x_new]/P[x_old]\n",
    "    \n",
    "    # Check whether accept or reject\n",
    "    \n",
    "    # Accept always\n",
    "    if A > 1:\n",
    "        x_samples.append(x_new)\n",
    "    \n",
    "    # Accept with probability A\n",
    "    else:\n",
    "        # Randomly decide to accept\n",
    "        r = np.random.rand()\n",
    "        if r < A:\n",
    "            x_samples.append(x_new)\n",
    "        else:\n",
    "            x_samples.append(x_old)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can check that we are getting sensible numbers out. Recall that the true mean and standard deviation of a Bernoulli distribution are $\\mu = p$ and $\\sigma = \\sqrt{p(1-p)}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The true mean is 0.7\n",
      "Our estimated mean is 0.69983\n",
      "The true standard deviation is 0.45825756949558405\n",
      "Our estimated standard deviation is 0.45833172604566663\n"
     ]
    }
   ],
   "source": [
    "print(\"The true mean is\", p)\n",
    "print(\"Our estimated mean is\", np.mean(x_samples))\n",
    "print(\"The true standard deviation is\", np.sqrt(p*(1-p)))\n",
    "print(\"Our estimated standard deviation is\", np.std(x_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We never explicitly specified the proposal distribution $Q$ in our code ($q$ does not appear anywhere). However, we did this implicitly in `numpy.random.choice` since by default it chooses between the two options with equal probability. This worked since we had chosen $q=0.5$.\n",
    "\n",
    "Notice that although we are sampling heads and tails equally (with `numpy.random.choice`), we are not getting out a mean $\\langle x \\rangle = 0.5$. The acceptance/rejection step is essential here for ensuring that our samples correspond to $P$ even though we are sampling from $Q$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=#46769B>Proof</font>\n",
    "\n",
    "Here we prove that MCMC algorithm really does generate samples for $P$. To be general, we will consider the more general Metropolis-Hastings algorithm. As above, we will consider a Bernoulli distribution to keep things simple. The arguments are similar with more complicated probability distributions, but we will not show them here.\n",
    "\n",
    "Suppose we have a given sample $x_i$. Let's calculate the probability the next sample $x_{i+1}$ is heads or tails according to the Metropolis-Hastings algorithm.\n",
    "\n",
    "First of all, we don't know whether $x_i$ is heads or tails. Let's assign a probability $f$ that $x_i$ is heads and $1-f$ that is tails. That is, if we ran many MCMC simulations, $x_i$ would be heads a fraction $f$ of the time and tails a fraction $1-f$ of the time. \n",
    "\n",
    "Now, there are three ways that $x_{i+1}$ could be heads:\n",
    "- $x_i$ is tails and there is a transition $0 \\to 1$ that is accepted.\n",
    "- $x_i$ is heads and there is a transition $1 \\to 1$. (Such a transition is always accepted since $\\mathcal{A}(1,1) = 1$.)\n",
    "- $x_i$ is heads and there is a transition $1 \\to 0$ that is rejected.\n",
    "\n",
    "The sum of these three probabilities is:\n",
    "$${\\rm Prob}(x_{i+1} = 1) = \\underbrace{(1-f)}_{{\\rm Prob.} \\; x_i = 0} \\times \\underbrace{Q(1|0)}_{{\\rm Prob.} \\; 0\\to 1} \n",
    "\\times \\underbrace{\\mathcal{A}(1,0)}_{{\\rm Prob.\\; accept}} + \n",
    "\\underbrace{f}_{{\\rm Prob.} \\; x_i = 1} \\times \\underbrace{Q(1|1)}_{{\\rm Prob.} \\; 1\\to 1} \n",
    "\\times \\underbrace{\\mathcal{A}(1,1)}_{{\\rm Prob.\\; accept}} + \n",
    "\\underbrace{f}_{{\\rm Prob.} \\; x_i = 1} \\times \\underbrace{Q(0|1)}_{{\\rm Prob.} \\; 1\\to 0} \n",
    "\\times \\underbrace{(1 - \\mathcal{A}(0,1))}_{{\\rm Prob.\\; reject}}$$\n",
    "Now we can plug in our expressions for $Q$ and $\\mathcal{A}$ given above:\n",
    "$${\\rm Prob}(x_{i+1} = 1) = (1-f)\\times q \\times {\\rm min}\\left( \\frac{p}{1-p} \\frac{1-\\bar q}{q}, 1 \\right) \n",
    "+ f \\times \\bar{q} + f \\times (1-\\bar q) \\times \\left(1 - {\\rm min}\\left( \\frac{1-p}{p} \\frac{q}{1-\\bar q}, 1 \\right)\\right) \\, .$$\n",
    "With some algebra we can write this as\n",
    "$${\\rm Prob}(x_{i+1} = 1) = f + \\left[ \\frac{1-f}{1-p} - \\frac{f}{p} \\right] \\times {\\rm min}\\left( p(1-\\bar q), q(1-p) \\right) \\, . \\qquad (1)$$\n",
    "\n",
    "The most interesting solutions to this equation are __stationary__ solutions. This means that the probability of finding heads for sample $x_{i+1}$ is equal to the probability of finding heads for $x_i$.\n",
    "That is, we set the above Eq. (1) equal to $f$:\n",
    "$${\\rm Prob}(x_{i+1} = 1) = f + \\left[ \\frac{1-f}{1-p} - \\frac{f}{p} \\right] \\times {\\rm min}\\left( p(1-\\bar q), q(1-p) \\right) = f \\, .$$\n",
    "The only solution is where the term in brackets is zero,\n",
    "$$\\frac{1-f}{1-p} - \\frac{f}{p}  = 0 \\, ,$$\n",
    "which requires that $f = p$.\n",
    "\n",
    "Let's paraphrase our arguments: \n",
    "- If we run our MCMC algorithm for many iterations, we expect our samples to converge to *some* unique stationary distribution, where the probability of getting heads or tails converges to a fixed value. (There are some technical requirements in order for this to be achieved; further reading can be found [here](https://similarweb.engineering/mcmc/).) \n",
    "- According to the Metropolis-Hastings algorithm (which includes the Metropolis algorithm as a special case), the stationary distribution is where a fraction $p$ of the samples are heads. (This implies obviously, though we did not show it explicitly, that a fraction $1-p$ of the samples are tails.)\n",
    "\n",
    "This is exactly what we wanted: our samples reproduce the target distribution $P(x)$.\n",
    "\n",
    "### <font color=#46769B>Caveat</font>\n",
    "\n",
    "The most important thing to remember about MCMCs is that *only the stationary distribution reproduces the target distribution $P(x)$*.\n",
    "\n",
    "To pick an extreme example, suppose we have a coin that is very biased toward heads, with $p=0.99$. But suppose we start our chain with tails, $x_0 = 0$, and run the Metropolis algorithm with $q = 0.01$. Since $Q(0|0) = 1-q = 99\\%$, we are very likely to sample many tails in a row before getting heads. Obviously, getting 50 or 100 tails in a row is not representative of the target probability distribution $P(x)$ that is strongly biased towards heads!\n",
    "\n",
    "While we are guaranteed to converge to a stationary distribution *eventually* that reproduces $P(x)$, the first samples of our chain many not reproduce $P(x)$ at all. In practice, the following rules apply:\n",
    "- One needs to discard the beginning of the chain, which is known as the __burn-in__ period. The burn-in period is simply the part of your chain where the algorithm is converging to its stationary distribution.\n",
    "- There are good proposal distributions and bad proposal distributions, and there are good initial points $x_0$ and bad ones. Good choices can make the burn-in period small or even zero, while bad choices can prevent your MCMC from burning-in within a reasonable amount of time.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
